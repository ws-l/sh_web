{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bLB244INhP0z"
      },
      "outputs": [],
      "source": [
        "!pip -q install fastapi uvicorn nest_asyncio pyngrok==7.2.0 \"pydantic<3\" joblib scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTHTOKEN = \"31dIffEzlumqxNYTNgdyPEwF6Xz_6DW1rQUikS4etab44vLrv\"  # 있으면 넣고, 없으면 빈 문자열 유지\n"
      ],
      "metadata": {
        "id": "zpS8ZivShRNl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRKM9xaAwY4g",
        "outputId": "0dc72c5a-5fe1-428d-c74f-2f6dfd68b1df"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.12/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, joblib, json\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import os, joblib, pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# ===== KoNLPy tokenizer (Okt) =====\n",
        "# 주의: joblib로 저장/재로딩할 때 이 함수 이름이 동일한 스코프에 존재해야 합니다.\n",
        "# (새 세션에서 로드 시에도 아래 함수 정의가 있어야 로드가 성공합니다)\n",
        "def tokenize_okt(text):\n",
        "    try:\n",
        "        from konlpy.tag import Okt\n",
        "        if not hasattr(tokenize_okt, \"_okt\"):\n",
        "            tokenize_okt._okt = Okt()\n",
        "        # 정규화/어간추출 포함, 불필요 기호 제거는 TF-IDF의 token_pattern에서 처리\n",
        "        return tokenize_okt._okt.morphs(str(text), norm=True, stem=True)\n",
        "    except Exception:\n",
        "        # KoNLPy 미설치/에러 시 공백 토큰화로 폴백 (성능 저하)\n",
        "        return str(text).split()\n",
        "\n",
        "# ===== 데이터 로딩 함수 =====\n",
        "def load_data_csv(path=\"data_sent_new.csv\", sample_n=2000, random_state=42):\n",
        "    # 인코딩은 utf-8-sig 우선, 실패 시 cp949 시도\n",
        "    encodings = [\"utf-8-sig\", \"utf-8\", \"cp949\"]\n",
        "    last_err = None\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise last_err\n",
        "\n",
        "    # 필수 컬럼 체크\n",
        "    required_cols = {\"sentence\", \"감정_대분류\"}\n",
        "    missing = required_cols - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"CSV에 {missing} 컬럼이 없습니다. 현재 컬럼: {list(df.columns)}\")\n",
        "\n",
        "    # 기쁨/불안만 사용\n",
        "    df = df[df[\"감정_대분류\"].isin([\"기쁨\", \"불안\"])].copy()\n",
        "    if len(df) == 0:\n",
        "        raise ValueError(\"기쁨/불안 라벨 행이 없습니다.\")\n",
        "\n",
        "    # 샘플링(2,000행, 행 수가 적으면 가능한 만큼만)\n",
        "    n = min(sample_n, len(df))\n",
        "    df = df.sample(n=n, random_state=random_state)\n",
        "\n",
        "    X = df[\"sentence\"].astype(str).tolist()\n",
        "    y = df[\"감정_대분류\"].astype(str).tolist()\n",
        "    return X, y\n",
        "\n",
        "MODEL_PATH = \"/content/model.pkl\"\n",
        "\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    # 1) 데이터 로드 & 전처리\n",
        "    X, y = load_data_csv(\"data_sent_new.csv\", sample_n=2000, random_state=42)\n",
        "\n",
        "    # 2) 파이프라인: TF-IDF(OKT 토크나이저) + 디시전트리\n",
        "    pipe = Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(\n",
        "            tokenizer=tokenize_okt,          # KoNLPy 기반 토크나이저\n",
        "            token_pattern=r\"(?u)\\b\\w+\\b\",    # 토크나이저가 있으므로 넓게 허용\n",
        "            ngram_range=(1, 2),              # 유니그램+바이그램\n",
        "            min_df=2,                        # 희귀 토큰 제거(필요시 조정)\n",
        "            max_df=0.98\n",
        "        )),\n",
        "        (\"clf\", DecisionTreeClassifier(\n",
        "            random_state=42,\n",
        "            # 필요시 하이퍼파라미터 예: max_depth=30, min_samples_leaf=3 등\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    # 3) 학습\n",
        "    pipe.fit(X, y)\n",
        "\n",
        "    # 4) 저장\n",
        "    joblib.dump(pipe, MODEL_PATH)\n",
        "    print(f\"모델 학습 및 저장 완료: {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"기존 모델을 로드합니다: {MODEL_PATH}\")\n",
        "\n",
        "# 5) 로딩(세션이 달라져서 토크나이저 오류가 난다면, 위의 tokenize_okt 정의가 반드시 있어야 함)\n",
        "pipe = joblib.load(MODEL_PATH)\n",
        "\n",
        "# 6) 확률 예측/클래스 확인\n",
        "supports_proba = hasattr(pipe, \"predict_proba\")\n",
        "label_classes = getattr(pipe, \"classes_\", None)\n",
        "print(\"모델 로딩 OK. supports_proba:\", supports_proba, \"classes:\", list(label_classes) if label_classes is not None else None)\n",
        "\n",
        "# --- 사용 예시 ---\n",
        "# samples = [\"새 프로젝트 생각만 해도 설렌다.\", \"면접 전날이라 너무 불안해.\"]\n",
        "# preds = pipe.predict(samples)\n",
        "# probs = pipe.predict_proba(samples) if supports_proba else None\n",
        "# print(preds, probs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPCEfjcshRQb",
        "outputId": "aff27503-8d92-479a-c177-3a88c23324b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 학습 및 저장 완료: /content/model.pkl\n",
            "모델 로딩 OK. supports_proba: True classes: [np.str_('기쁨'), np.str_('불안')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI(title=\"TextClassifier API\", version=\"1.0.0\", description=\"PKL 모형 기반 문장 분류 API\")\n",
        "# CORS: 필요시 Custom GPT 도메인 허용(여기선 모두 허용)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"], allow_credentials=True,\n",
        "    allow_methods=[\"*\"], allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# ------- Request/Response 스키마 -------\n",
        "class PredictRequest(BaseModel):\n",
        "    text: str | None = None\n",
        "    texts: list[str] | None = None  # 배치 요청 지원\n",
        "\n",
        "class PredictResponse(BaseModel):\n",
        "    labels: list[str]\n",
        "    probabilities: list[dict] | None = None  # 각 문장별 {라벨: 확률}\n",
        "\n",
        "# ------- 헬스체크 -------\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"ok\"}\n",
        "\n",
        "# ------- 예측 엔드포인트 -------\n",
        "@app.post(\"/predict\", response_model=PredictResponse)\n",
        "def predict(req: PredictRequest):\n",
        "    # 단문 또는 배치 둘 다 지원\n",
        "    inputs = []\n",
        "    if req.text is not None:\n",
        "        inputs = [req.text]\n",
        "    elif req.texts is not None and len(req.texts) > 0:\n",
        "        inputs = req.texts\n",
        "    else:\n",
        "        return PredictResponse(labels=[], probabilities=[])\n",
        "\n",
        "    preds = pipe.predict(inputs)\n",
        "    labels = [str(p) for p in preds]\n",
        "\n",
        "    probas = None\n",
        "    if supports_proba:\n",
        "        raw = pipe.predict_proba(inputs)  # shape: (n_samples, n_classes)\n",
        "        classes = list(pipe.classes_) if label_classes is None else label_classes\n",
        "        probas = []\n",
        "        for row in raw:\n",
        "            probas.append({cls: float(p) for cls, p in zip(classes, row)})\n",
        "\n",
        "    return PredictResponse(labels=labels, probabilities=probas)\n",
        "\n",
        "# --- Uvicorn을 백그라운드로 기동하는 헬퍼 ---\n",
        "def run_uvicorn():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\", access_log=False)\n"
      ],
      "metadata": {
        "id": "_kkS5ja-hYeE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading, time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# ngrok 준비\n",
        "if NGROK_AUTHTOKEN:\n",
        "    ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# 기존 터널 정리\n",
        "for t in ngrok.get_tunnels():\n",
        "    ngrok.disconnect(t.public_url)\n",
        "\n",
        "# 새 HTTPS 터널\n",
        "public_tunnel = ngrok.connect(8000, \"http\")\n",
        "public_url = public_tunnel.public_url.replace(\"http://\", \"https://\")  # https로 통일\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# servers 주입용 OpenAPI 오버라이드\n",
        "from fastapi.openapi.utils import get_openapi\n",
        "\n",
        "def custom_openapi():\n",
        "    if app.openapi_schema:\n",
        "        # 기존에 생성되어 있다면 servers만 바꿔치기\n",
        "        app.openapi_schema[\"servers\"] = [{\"url\": public_url}]\n",
        "        return app.openapi_schema\n",
        "\n",
        "    openapi_schema = get_openapi(\n",
        "        title=app.title,\n",
        "        version=app.version,\n",
        "        description=app.description,\n",
        "        routes=app.routes,\n",
        "    )\n",
        "    openapi_schema[\"servers\"] = [{\"url\": public_url}]\n",
        "    app.openapi_schema = openapi_schema\n",
        "    return app.openapi_schema\n",
        "\n",
        "app.openapi = custom_openapi  # 동적 바인딩\n",
        "\n",
        "# Uvicorn 백그라운드 기동\n",
        "thread = threading.Thread(target=run_uvicorn, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(1.5)  # 서버 부팅 대기\n",
        "\n",
        "print(\"Docs:\", public_url + \"/docs\")\n",
        "print(\"OpenAPI JSON:\", public_url + \"/openapi.json\")\n",
        "print(\"Health:\", public_url + \"/health\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIJKYKWRhYhH",
        "outputId": "d4b5d11b-4265-4a3a-a3bf-6e3971b96d9c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-22T08:04:57+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-e7a6de50-8a99-498a-a33f-4487c69cc4a0 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "INFO:     Started server process [378]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): [errno 98] address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://30320357d6a0.ngrok.app\n",
            "Docs: https://30320357d6a0.ngrok.app/docs\n",
            "OpenAPI JSON: https://30320357d6a0.ngrok.app/openapi.json\n",
            "Health: https://30320357d6a0.ngrok.app/health\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json\n",
        "\n",
        "sample = {\"texts\": [\"오늘 정말 행복해!\", \"불안하고 걱정돼.\"]}\n",
        "r = requests.post(public_url + \"/predict\", headers={\"Content-Type\":\"application/json\"}, data=json.dumps(sample))\n",
        "print(r.status_code)\n",
        "print(r.json())\n"
      ],
      "metadata": {
        "id": "JRORzN9jhRVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "952eeeda-2114-47da-dbad-adcadde3d154"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n",
            "{'labels': ['기쁨', '불안'], 'probabilities': [{'기쁨': 1.0, '불안': 0.0}, {'기쁨': 0.0, '불안': 1.0}]}\n"
          ]
        }
      ]
    }
  ]
}